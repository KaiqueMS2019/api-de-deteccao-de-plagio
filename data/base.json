[
  {
    "texto": "A inteligência artificial é a simulação de processos humanos por máquinas.",
    "fonte": "artigo1"
  },
  {
    "texto": "Python é muito utilizado em machine learning e NLP, por conta de libs e frameworks que facilitam muito.",
    "fonte": "artigo2"
  },
  {
    "texto": "Redações escolares são corrigidas com base em coerência e gramática.",
    "fonte": "artigo3"
  },
  {
    "texto": "Modelos de linguagem de grande escala (em inglês: Large Language Model ou LLM)são modelos de linguagem compostos por uma rede neural com muitos parâmetros (tipicamente bilhões ou possivelmente mais). São treinados com grandes quantidades de textos não rotulado usando aprendizado de máquina não-supervisionado. Os LLM surgiram por volta de 2018,[6] com o modelo BERT.[6] Estes têm bom desempenho em uma ampla variedade de tarefas. Isso mudou o foco da pesquisa em processamento de linguagem natural, afastando-se do paradigma anterior de treinar modelos supervisionados especializados para tarefas específicas.\r\n\r\nPropriedades\r\nEmbora o termo não tenha uma definição formal, ele geralmente se refere a modelos de aprendizado profundo que possuem uma contagem de parâmetros da ordem de bilhões ou mais. MLLs são modelos de propósito geral que se destacam em uma ampla gama de tarefas, em vez de serem treinados para uma tarefa específica (como análise de sentimento, reconhecimento de entidades nomeadas ou raciocínio matemático). Embora treinados em tarefas simples, como prever a próxima palavra em uma frase, modelos de linguagem neural com treinamento e contagem de parâmetros suficientes são capazes de capturar grande parte da sintaxe e semântica da linguagem humana. Além disso, grandes modelos de linguagem demonstram considerável conhecimento geral sobre o mundo e são capazes de \"memorizar\" uma grande quantidade de fatos durante o treinamento.\r\n\r\nArquitetura e treinamento\r\nOs modelos grandes de linguagem geralmente usam a arquitetura do transformador, que se tornou a técnica padrão de aprendizado profundo para dados sequenciais desde 2018 (anteriormente, arquiteturas recorrentes como a LSTM eram mais comuns). Os MLLs são treinados de maneira não supervisionada em texto não anotado. Um transformador da esquerda para a direita é treinado para maximizar a probabilidade atribuída à próxima palavra nos dados de treinamento, dada o contexto anterior. Alternativamente, um MLL pode usar um transformador bidirecional (como no exemplo do BERT), que atribui uma distribuição de probabilidade sobre as palavras com acesso tanto ao contexto anterior quanto posterior. Além da tarefa de prever a próxima palavra ou \"preencher as lacunas\", os MLLs podem ser treinados em tarefas auxiliares que testam sua compreensão da distribuição de dados, como a Predição de Próxima Sentença (NSP), na qual pares de frases são apresentados e o modelo deve prever se eles aparecem lado a lado no corpus de treinamento.\r\n\r\nOs primeiros MLLs foram treinados em corpora com bilhões de palavras. A versão inicial do GPT foi treinada em 2018 no BookCorpus, que consiste em 985 milhões de palavras. No mesmo ano, o BERT foi treinado em uma combinação de BookCorpus e Wikipedia em inglês, totalizando 3,3 bilhões de palavras. Nos anos seguintes, os corpora de treinamento para MLLs aumentaram em ordens de magnitude, atingindo centenas de bilhões ou trilhões de tokens.\r\n\r\nMLLs são caros em termos computacionais para serem treinados. Um estudo de 2020 estimou o custo do treinamento de um modelo com 1.5 bilhão de parâmetros (de 1 a 2 ordens de magnitude menores que o estado da arte na época) em US$ 1.6 milhão.\r\n\r\nUma análise de 2020 constatou que a capacidade dos modelos de linguagem neural (medida pela perda de treinamento) aumentou suavemente em uma relação de lei de potência com o número de parâmetros, quantidade de dados de treinamento e computação usada para treinamento. Essas relações foram testadas em uma ampla gama de valores (até sete ordens de magnitude) e nenhuma atenuação da relação foi observada no extremo superior da faixa (incluindo tamanhos de rede de até trilhões de parâmetros).\r\n\r\nAplicação em tarefas secundárias\r\nEntre 2018 e 2020, o método padrão para utilizar um MLL em uma tarefa específica de processamento de linguagem natural era ajustar o modelo com treinamento adicional específico para a tarefa. Posteriormente, descobriu-se que MLLs mais poderosos, como o GPT-3, podem resolver tarefas sem treinamento adicional por meio de técnicas de \"prompting\", nas quais o problema a ser resolvido é apresentado ao modelo como um prompt de texto, possivelmente com alguns exemplos textuais de problemas semelhantes e suas soluções.\r\n\r\nAjuste fino\r\nO ajuste fino é a prática de modificar um modelo de linguagem pré-treinado existente, treinando-o (de forma supervisionada) em uma tarefa específica (por exemplo, análise de sentimento, reconhecimento de entidade nomeada ou marcação de partes do discurso). É uma forma de aprendizado por transferência. Geralmente envolve a introdução de um novo conjunto de pesos conectando a camada final do modelo de linguagem à saída da tarefa secundária. Os pesos originais do modelo de linguagem podem ser \"congelados\", de modo que apenas a nova camada de pesos conectando-os à saída seja aprendida durante o treinamento. Alternativamente, os pesos originais podem receber pequenas atualizações (possivelmente com camadas anteriores congeladas).",
    "fonte": "llm base.txt"
  },
  {
    "texto": "A história do PLN começou na década de 1950, quando Alan Turing publicou o artigo \"Computing Machinery and Intelligence\", que propunha o que agora é chamado de teste de Turing como critério de inteligência.\r\n\r\nEm 1954, a experiência de Georgetown envolveu a tradução automática de mais de sessenta frases russas para o inglês. Os autores afirmaram que dentro de três ou cinco anos a tradução automática seria um problema resolvido.[2] No entanto, os avanços reais foram muito mais lentos do que o previsto e, após o relatório ALPAC em 1966, que constatou que a pesquisa de dez anos não conseguiu satisfazer as expectativas, o financiamento para este estudo em tradução automática foi reduzido drasticamente. Poucas pesquisas em tradução automática foram conduzidas até o final dos anos 80, quando os primeiros sistemas estatísticos de tradução foram desenvolvidos.\r\n\r\nAlguns sistemas de PLN bem sucedidos desenvolvidos nos anos 60 foram SHRDLU, um sistema de língua natural que trabalhava em \"blocks worlds\" com vocabulário restrito e ELIZA, uma simulação de um psicoterapeuta escrita por Joseph Weizenbaum entre 1964 e 1966. Usando pouca informação sobre o pensamento ou a emoção humana, ELIZA criava, em alguns casos, interações surpreendentemente humanas. Quando o \"paciente\" excedia a base de conhecimento do programa, ELIZA fornecia uma resposta genérica, por exemplo, respondendo a \"Minha cabeça dói\" com \"Por que você diz que sua cabeça dói?\".\r\n\r\nDurante a década de 1970, muitos programadores começaram a escrever \"ontologias conceituais\", que estruturaram a informação do mundo real em dados compreensíveis por computadores. Exemplos são MARGIE (SCHANK, 1975), SAM (CULLINGFORD, 1978), PAM (WILENSKY, 1978), TaleSpin (MEEHAN, 1976), QUALM (LEHNERT, 1977), Politics (CARBONELL, 1979) e Plot Units (LEHNERT, 1981 ). Neste período, muitos chatterbots foram escritos, como PARRY, Racter e Jabberwacky.\r\n\r\nAté a década de 1980, a maioria dos sistemas de PLN se baseava em conjuntos complexos de regras manuscritas. A partir do final dos anos 1980, no entanto, houve uma revolução no PLN com a introdução de algoritmos de aprendizagem automática (aprendizado de máquina) para o processamento de linguagem. Isto foi devido tanto ao aumento constante do poder computacional (ver Lei de Moore) quanto à diminuição gradual da dominância das teorias da linguística chomskyanas (como a gramática gerativa), cujos fundamentos teóricos desestimularam o tipo de corpus linguístico que está subjacente à abordagem da aprendizagem automática ao processamento da linguagem[3].\r\n\r\nAlguns dos algoritmos de aprendizado de máquinas mais antigos, como as árvores de decisão, produziam sistemas de regras rígidas então semelhantes às regras existentes na escritas à mão. No entanto, a marcação de partes da fala (part-of-speech tagging) introduziu o uso de modelos ocultos de Markov para o PLN e, cada vez mais, a pesquisa se concentrava em modelos estatísticos, que tomam decisões suaves e probabilísticas baseadas na atribuição de pesos reais aos recursos que compõem dados de entrada. Os modelos de linguagem de cache, sobre os quais muitos sistemas de reconhecimento de fala agora dependem, são exemplos de tais modelos estatísticos. Esses modelos são geralmente mais robustos quando dados informações desconhecidas, especialmente entrada que contém erros (como é muito comum para dados do mundo real) e produzem resultados mais confiáveis quando integrados em sistemas maiores que compreendem múltiplas tarefas.",
    "fonte": "A história do PLN.txt"
  },
  {
    "texto": "Uma máquina de vetores de suporte (SVM, do inglês: support-vector machine) é um conceito na ciência da computação para um conjunto de métodos de aprendizado supervisionado (aprendizagem de máquina supervisionada) que analisam os dados e reconhecem padrões, usado para classificação e análise de regressão. O SVM padrão toma como entrada um conjunto de dados e prediz, para cada entrada dada, qual de duas possíveis classes a entrada faz parte, o que faz do SVM um classificador linear binário não probabilístico. Dados um conjunto de exemplos de treinamento, cada um marcado como pertencente a uma de duas categorias, um algoritmo de treinamento do SVM constrói um modelo que atribui novos exemplos a uma categoria ou outra. Um modelo SVM é uma representação de exemplos como pontos no espaço, mapeados de maneira que os exemplos de cada categoria sejam divididos por um espaço claro que seja tão amplo quanto possível. Os novos exemplos são então mapeados no mesmo espaço e preditos como pertencentes a uma categoria baseados em qual o lado do espaço eles são colocados.\r\n\r\nEm outras palavras, o que uma SVM faz é encontrar uma linha de separação, mais comumente chamada de hiperplano entre dados de duas classes.",
    "fonte": "Uma máquina de vetores de suporte.txt"
  }
]